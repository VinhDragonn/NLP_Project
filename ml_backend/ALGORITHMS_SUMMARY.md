# T·ªïng h·ª£p c√°c thu·∫≠t to√°n NLP ƒë√£ tri·ªÉn khai

## üìö Danh s√°ch thu·∫≠t to√°n

### 1. Text Preprocessing (Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n)

#### 1.1 Tokenization
**File**: `nlp_preprocessing.py` - Class `VietnameseTokenizer`

**M√¥ t·∫£**: T√°ch vƒÉn b·∫£n th√†nh c√°c token (t·ª´)

**Thu·∫≠t to√°n**:
```python
def tokenize(text):
    1. Chuy·ªÉn text v·ªÅ lowercase
    2. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (gi·ªØ l·∫°i d·∫•u ti·∫øng Vi·ªát)
    3. T√°ch theo kho·∫£ng tr·∫Øng
    4. Lo·∫°i b·ªè token r·ªóng
    return tokens
```

**V√≠ d·ª•**:
```
Input:  "T√¨m phim h√†nh ƒë·ªông!"
Output: ["t√¨m", "phim", "h√†nh", "ƒë·ªông"]
```

---

#### 1.2 Porter Stemmer
**File**: `nlp_preprocessing.py` - Class `PorterStemmer`

**M√¥ t·∫£**: C·∫Øt b·ªè h·∫≠u t·ªë ƒë·ªÉ ƒë∆∞a t·ª´ v·ªÅ d·∫°ng g·ªëc (ti·∫øng Anh)

**Thu·∫≠t to√°n**:
```python
def stem(word):
    1. X√°c ƒë·ªãnh measure m c·ªßa t·ª´ (s·ªë l∆∞·ª£ng chu·ªói VC)
    2. Lo·∫°i b·ªè plurals (sses ‚Üí ss, ies ‚Üí i, s ‚Üí ‚àÖ)
    3. Lo·∫°i b·ªè past tense (eed, ed, ing)
    4. Lo·∫°i b·ªè double consonants
    return stemmed_word
```

**C√¥ng th·ª©c measure**:
```
Word = [C](VC)^m[V]
m = s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa pattern VC
```

**V√≠ d·ª•**:
```
running ‚Üí run
movies ‚Üí movi
played ‚Üí play
```

---

#### 1.3 Stop Words Removal
**File**: `nlp_preprocessing.py` - Class `StopWordsRemover`

**M√¥ t·∫£**: Lo·∫°i b·ªè t·ª´ d·ª´ng (stop words) kh√¥ng mang nhi·ªÅu √Ω nghƒ©a

**Danh s√°ch stop words**:
- Ti·∫øng Vi·ªát: v√†, c·ªßa, c√≥, ƒë∆∞·ª£c, trong, l√†, v·ªõi, cho...
- Ti·∫øng Anh: the, a, an, and, or, but, in, on, at...

**V√≠ d·ª•**:
```
Input:  ["t√¨m", "phim", "h√†nh", "ƒë·ªông", "c·ªßa", "t√¥i"]
Output: ["phim", "h√†nh", "ƒë·ªông"]
```

---

### 2. TF-IDF (Term Frequency - Inverse Document Frequency)

**File**: `nlp_preprocessing.py` - Class `TFIDFVectorizer`

**M√¥ t·∫£**: T√≠nh tr·ªçng s·ªë c·ªßa t·ª´ trong t√†i li·ªáu

**C√¥ng th·ª©c to√°n h·ªçc**:

```
TF(t,d) = count(t in d) / total_words(d)

IDF(t) = log(N / (df(t) + 1))
    N = t·ªïng s·ªë documents
    df(t) = s·ªë documents ch·ª©a term t

TF-IDF(t,d) = TF(t,d) √ó IDF(t)
```

**Thu·∫≠t to√°n**:
```python
def fit(documents):
    1. X√¢y d·ª±ng vocabulary t·ª´ t·∫•t c·∫£ documents
    2. T√≠nh document frequency cho m·ªói term
    3. T√≠nh IDF cho m·ªói term: log(N / (df + 1))

def transform(document):
    1. T√≠nh term frequency cho document
    2. Nh√¢n TF v·ªõi IDF
    3. Return TF-IDF vector
```

**V√≠ d·ª•**:
```
Documents:
  D1: "action movie"
  D2: "comedy movie"
  D3: "action film"

TF-IDF cho "action" trong D1:
  TF = 1/2 = 0.5
  IDF = log(3/2) = 0.176
  TF-IDF = 0.5 √ó 0.176 = 0.088
```

---

### 3. Cosine Similarity (ƒê·ªô t∆∞∆°ng ƒë·ªìng Cosine)

**File**: `nlp_preprocessing.py`, `nlp_semantic_similarity.py`

**M√¥ t·∫£**: T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa 2 vectors

**C√¥ng th·ª©c to√°n h·ªçc**:

```
cos(A, B) = (A ¬∑ B) / (||A|| √ó ||B||)

A ¬∑ B = Œ£(Ai √ó Bi)  (dot product)

||A|| = ‚àö(Œ£(Ai¬≤))  (magnitude)
```

**Thu·∫≠t to√°n**:
```python
def cosine_similarity(vec1, vec2):
    1. T√¨m common keys gi·ªØa 2 vectors
    2. T√≠nh dot product: Œ£(vec1[k] √ó vec2[k])
    3. T√≠nh magnitude c·ªßa vec1: ‚àö(Œ£(vec1[k]¬≤))
    4. T√≠nh magnitude c·ªßa vec2: ‚àö(Œ£(vec2[k]¬≤))
    5. Return dot_product / (mag1 √ó mag2)
```

**V√≠ d·ª•**:
```
vec1 = {action: 0.5, movie: 0.3}
vec2 = {action: 0.4, movie: 0.6}

dot_product = 0.5√ó0.4 + 0.3√ó0.6 = 0.38
mag1 = ‚àö(0.5¬≤ + 0.3¬≤) = 0.583
mag2 = ‚àö(0.4¬≤ + 0.6¬≤) = 0.721
similarity = 0.38 / (0.583 √ó 0.721) = 0.904
```

---

### 4. Naive Bayes Classifier

**File**: `nlp_intent_classifier.py` - Class `NaiveBayesClassifier`

**M√¥ t·∫£**: Ph√¢n lo·∫°i vƒÉn b·∫£n d·ª±a tr√™n x√°c su·∫•t Bayes

**C√¥ng th·ª©c to√°n h·ªçc**:

```
P(class|document) = P(class) √ó ‚àè P(word|class)

P(class) = count(class) / total_documents

P(word|class) v·ªõi Laplace Smoothing:
P(word|class) = (count(word, class) + 1) / (count(class) + |V|)
    |V| = vocabulary size
```

**Thu·∫≠t to√°n**:

```python
def train(documents, labels):
    1. T√≠nh P(class) cho m·ªói class
    2. ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói word trong m·ªói class
    3. T√≠nh P(word|class) v·ªõi Laplace smoothing

def predict(document):
    1. V·ªõi m·ªói class:
        score = log(P(class))
        V·ªõi m·ªói word trong document:
            score += log(P(word|class))
    2. Return class c√≥ score cao nh·∫•t
```

**V√≠ d·ª•**:
```
Training data:
  "action movie" ‚Üí search_by_genre
  "find movie" ‚Üí search_by_title
  "action film" ‚Üí search_by_genre

Predict: "action movie"
  P(search_by_genre|doc) = P(search_by_genre) √ó P(action|genre) √ó P(movie|genre)
  P(search_by_title|doc) = P(search_by_title) √ó P(action|title) √ó P(movie|title)
  
  ‚Üí Choose class with higher probability
```

---

### 5. Support Vector Machine (SVM)

**File**: `nlp_intent_classifier.py` - Class `SimpleSVM`

**M√¥ t·∫£**: Ph√¢n lo·∫°i b·∫±ng c√°ch t√¨m hyperplane t·ªëi ∆∞u

**C√¥ng th·ª©c to√°n h·ªçc**:

```
Decision function: f(x) = w¬∑x + b

Hinge Loss: L = max(0, 1 - y √ó f(x))

Weight update (Gradient Descent):
  if margin < 1:
      w = w + Œ± √ó (y √ó x - 2Œª √ó w)
      b = b + Œ± √ó y
  else:
      w = w + Œ± √ó (-2Œª √ó w)
```

**Thu·∫≠t to√°n**:

```python
def train(documents, labels):
    1. Chuy·ªÉn documents th√†nh feature vectors
    2. V·ªõi m·ªói class (one-vs-rest):
        a. T·∫°o binary labels (1 cho class, -1 cho others)
        b. Kh·ªüi t·∫°o weights w v√† bias b
        c. Gradient descent:
            V·ªõi m·ªói sample (x, y):
                margin = y √ó (w¬∑x + b)
                if margin < 1:
                    w += Œ± √ó (y√óx - 2Œª√ów)
                    b += Œ± √ó y
                else:
                    w += Œ± √ó (-2Œª√ów)

def predict(document):
    1. Chuy·ªÉn document th√†nh feature vector
    2. T√≠nh score cho m·ªói class: w¬∑x + b
    3. Return class c√≥ score cao nh·∫•t
```

**V√≠ d·ª•**:
```
Training v·ªõi 2 classes:
  Class A: [1, 0, 1] ‚Üí 1
  Class B: [0, 1, 0] ‚Üí -1

Sau training:
  w = [0.5, -0.3, 0.4]
  b = 0.1

Predict [1, 0, 1]:
  score = 0.5√ó1 + (-0.3)√ó0 + 0.4√ó1 + 0.1 = 1.0
  ‚Üí Class A
```

---

### 6. Levenshtein Distance (Edit Distance)

**File**: `nlp_semantic_similarity.py` - Class `LevenshteinDistance`

**M√¥ t·∫£**: T√≠nh kho·∫£ng c√°ch ch·ªânh s·ª≠a gi·ªØa 2 chu·ªói

**C√¥ng th·ª©c to√°n h·ªçc**:

```
D[i,j] = min(
    D[i-1,j] + 1,        # deletion
    D[i,j-1] + 1,        # insertion
    D[i-1,j-1] + cost    # substitution
)

cost = 0 if s1[i] == s2[j] else 1
```

**Thu·∫≠t to√°n (Dynamic Programming)**:

```python
def calculate(s1, s2):
    1. T·∫°o matrix D[len(s1)+1][len(s2)+1]
    2. Kh·ªüi t·∫°o h√†ng ƒë·∫ßu v√† c·ªôt ƒë·∫ßu: D[i,0]=i, D[0,j]=j
    3. V·ªõi m·ªói i t·ª´ 1 ƒë·∫øn len(s1):
        V·ªõi m·ªói j t·ª´ 1 ƒë·∫øn len(s2):
            if s1[i-1] == s2[j-1]:
                cost = 0
            else:
                cost = 1
            D[i,j] = min(
                D[i-1,j] + 1,      # delete
                D[i,j-1] + 1,      # insert
                D[i-1,j-1] + cost  # substitute
            )
    4. Return D[len(s1)][len(s2)]
```

**V√≠ d·ª•**:
```
s1 = "kitten"
s2 = "sitting"

Matrix D:
      ""  s  i  t  t  i  n  g
  ""   0  1  2  3  4  5  6  7
  k    1  1  2  3  4  5  6  7
  i    2  2  1  2  3  4  5  6
  t    3  3  2  1  2  3  4  5
  t    4  4  3  2  1  2  3  4
  e    5  5  4  3  2  2  3  4
  n    6  6  5  4  3  3  2  3

Distance = 3
Operations: k‚Üís, e‚Üíi, insert g
```

---

### 7. Jaccard Similarity

**File**: `nlp_semantic_similarity.py` - Class `JaccardSimilarity`

**M√¥ t·∫£**: T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa 2 t·∫≠p h·ª£p

**C√¥ng th·ª©c to√°n h·ªçc**:

```
J(A, B) = |A ‚à© B| / |A ‚à™ B|

|A ‚à© B| = s·ªë ph·∫ßn t·ª≠ chung
|A ‚à™ B| = t·ªïng s·ªë ph·∫ßn t·ª≠ (kh√¥ng tr√πng)
```

**Thu·∫≠t to√°n**:

```python
def calculate(set1, set2):
    1. T√≠nh intersection: set1 ‚à© set2
    2. T√≠nh union: set1 ‚à™ set2
    3. Return |intersection| / |union|
```

**V√≠ d·ª•**:
```
A = {action, movie, 2024}
B = {action, film, 2024}

A ‚à© B = {action, 2024}  ‚Üí size = 2
A ‚à™ B = {action, movie, film, 2024}  ‚Üí size = 4

J(A,B) = 2/4 = 0.5
```

---

### 8. N-gram Similarity

**File**: `nlp_semantic_similarity.py` - Class `NGramSimilarity`

**M√¥ t·∫£**: T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng d·ª±a tr√™n n-grams k√Ω t·ª±

**Thu·∫≠t to√°n**:

```python
def get_ngrams(text, n):
    1. Chuy·ªÉn text v·ªÅ lowercase
    2. T·∫°o n-grams: [text[i:i+n] for i in range(len(text)-n+1)]
    3. Return set of n-grams

def calculate(text1, text2, n):
    1. T·∫°o n-grams cho text1
    2. T·∫°o n-grams cho text2
    3. T√≠nh Jaccard similarity gi·ªØa 2 sets n-grams
```

**V√≠ d·ª• (bigrams, n=2)**:
```
text1 = "action"
text2 = "actor"

bigrams1 = {ac, ct, ti, io, on}
bigrams2 = {ac, ct, to, or}

Common = {ac, ct}  ‚Üí size = 2
Union = {ac, ct, ti, io, on, to, or}  ‚Üí size = 7

Similarity = 2/7 = 0.286
```

---

### 9. Word Embeddings (Co-occurrence)

**File**: `nlp_semantic_similarity.py` - Class `WordEmbedding`

**M√¥ t·∫£**: T·∫°o vector bi·ªÉu di·ªÖn t·ª´ d·ª±a tr√™n co-occurrence

**Thu·∫≠t to√°n**:

```python
def train(documents, window_size):
    1. X√¢y d·ª±ng vocabulary
    2. T·∫°o co-occurrence matrix:
        V·ªõi m·ªói document:
            V·ªõi m·ªói word w:
                V·ªõi m·ªói context word c trong window:
                    cooccurrence[w][c] += 1
    3. Chuy·ªÉn co-occurrence th√†nh vectors:
        V·ªõi m·ªói word:
            L·∫•y top-k co-occurring words
            Normalize vector
    4. Return word vectors

def similarity(word1, word2):
    1. L·∫•y vector c·ªßa word1 v√† word2
    2. T√≠nh cosine similarity
```

**V√≠ d·ª•**:
```
Documents:
  "action movie good"
  "action film great"
  "comedy movie funny"

Co-occurrence (window=1):
  action: {movie: 1, film: 1}
  movie: {action: 1, good: 1, comedy: 1, funny: 1}
  film: {action: 1, great: 1}

Vector cho "action":
  {movie: 0.707, film: 0.707}  (normalized)
```

---

### 10. Spell Correction

**File**: `nlp_query_expansion.py` - Class `SpellCorrector`

**M√¥ t·∫£**: S·ª≠a l·ªói ch√≠nh t·∫£ d·ª±a tr√™n edit distance v√† frequency

**Thu·∫≠t to√°n**:

```python
def correct(word):
    1. N·∫øu word trong vocabulary ‚Üí return word
    2. T·∫°o candidates v·ªõi edit distance = 1:
        - Deletions: b·ªè 1 k√Ω t·ª±
        - Transpositions: ƒë·ªïi ch·ªó 2 k√Ω t·ª± li·ªÅn k·ªÅ
        - Replacements: thay 1 k√Ω t·ª±
        - Insertions: th√™m 1 k√Ω t·ª±
    3. L·ªçc candidates c√≥ trong vocabulary
    4. N·∫øu kh√¥ng c√≥, t·∫°o candidates v·ªõi edit distance = 2
    5. Return candidate c√≥ frequency cao nh·∫•t
```

**V√≠ d·ª•**:
```
word = "acton"
vocabulary = {action, actor, act}

Edits distance 1:
  - Deletions: cton, aton, acon, actn, acto
  - Transpositions: caton, atcon, acton, actno
  - Replacements: bcton, ccton, ..., actio, ...
  - Insertions: aacton, bacton, ..., actionn, ...

Candidates in vocabulary: {action, actor}

Frequencies:
  action: 100
  actor: 50

Return: "action"
```

---

### 11. Query Expansion

**File**: `nlp_query_expansion.py` - Class `QueryExpander`

**M√¥ t·∫£**: M·ªü r·ªông query v·ªõi synonyms, hypernyms, hyponyms

**Thu·∫≠t to√°n**:

```python
def expand_with_synonyms(query):
    1. Tokenize query
    2. V·ªõi m·ªói token:
        if token c√≥ synonyms:
            T·∫°o expanded queries b·∫±ng c√°ch thay token b·∫±ng synonyms
    3. Return list of expanded queries

def expand_with_hypernyms(query):
    1. Tokenize query
    2. V·ªõi m·ªói token:
        if token c√≥ hypernym (t·ª´ t·ªïng qu√°t h∆°n):
            T·∫°o expanded query v·ªõi hypernym
    3. Return expanded queries

def expand_with_hyponyms(query):
    1. Tokenize query
    2. V·ªõi m·ªói token:
        if token c√≥ hyponyms (t·ª´ c·ª• th·ªÉ h∆°n):
            T·∫°o expanded queries v·ªõi hyponyms
    3. Return expanded queries
```

**V√≠ d·ª•**:
```
Query: "good action movie"

Synonyms:
  good ‚Üí [great, excellent, amazing]
  movie ‚Üí [film, cinema]

Expanded:
  - "great action movie"
  - "excellent action movie"
  - "good action film"
  - "great action film"

Hypernyms:
  action ‚Üí movie

Expanded:
  - "good movie"

Hyponyms:
  movie ‚Üí [action, comedy, horror]

Expanded:
  - "good action movie action"
  - "good action movie comedy"
```

---

## üìä T·ªïng k·∫øt

### S·ªë l∆∞·ª£ng thu·∫≠t to√°n: **11 thu·∫≠t to√°n ch√≠nh**

1. ‚úÖ Tokenization
2. ‚úÖ Porter Stemmer
3. ‚úÖ TF-IDF
4. ‚úÖ Cosine Similarity
5. ‚úÖ Naive Bayes
6. ‚úÖ SVM (Gradient Descent)
7. ‚úÖ Levenshtein Distance
8. ‚úÖ Jaccard Similarity
9. ‚úÖ N-gram Similarity
10. ‚úÖ Word Embeddings
11. ‚úÖ Spell Correction

### ƒê·ªô ph·ª©c t·∫°p thu·∫≠t to√°n

| Thu·∫≠t to√°n | Time Complexity | Space Complexity |
|-----------|----------------|------------------|
| Tokenization | O(n) | O(n) |
| Porter Stemmer | O(n) | O(1) |
| TF-IDF | O(n√óm) | O(v) |
| Cosine Similarity | O(min(v1,v2)) | O(1) |
| Naive Bayes | O(n√óm) train, O(m) predict | O(v√óc) |
| SVM | O(n√óm√ói) | O(m√óc) |
| Levenshtein | O(n√óm) | O(n√óm) |
| Jaccard | O(n+m) | O(n+m) |
| N-gram | O(n+m) | O(n+m) |
| Word Embeddings | O(n√ów√ód) | O(v√ód) |
| Spell Correction | O(n√ó26^2) | O(v) |

Trong ƒë√≥:
- n, m: ƒë·ªô d√†i chu·ªói/document
- v: vocabulary size
- c: s·ªë classes
- i: s·ªë iterations
- w: window size
- d: embedding dimension

### Files t∆∞∆°ng ·ª©ng

```
ml_backend/
‚îú‚îÄ‚îÄ nlp_preprocessing.py          # Algorithms 1-4
‚îú‚îÄ‚îÄ nlp_intent_classifier.py      # Algorithms 5-6
‚îú‚îÄ‚îÄ nlp_semantic_similarity.py    # Algorithms 7-10
‚îú‚îÄ‚îÄ nlp_query_expansion.py        # Algorithm 11
‚îú‚îÄ‚îÄ nlp_ner.py                    # Entity Recognition
‚îú‚îÄ‚îÄ nlp_service.py                # FastAPI Service
‚îî‚îÄ‚îÄ test_nlp_algorithms.py        # Test all algorithms
```

## üéì √Åp d·ª•ng ki·∫øn th·ª©c m√¥n h·ªçc

T·∫•t c·∫£ c√°c thu·∫≠t to√°n tr√™n ƒë·ªÅu ƒë∆∞·ª£c code t·ª´ ƒë·∫ßu (from scratch) ƒë·ªÉ:

1. **Hi·ªÉu r√µ c√°ch ho·∫°t ƒë·ªông** c·ªßa t·ª´ng thu·∫≠t to√°n
2. **√Åp d·ª•ng ki·∫øn th·ª©c** m√¥n Ng√¥n ng·ªØ t·ª± nhi√™n
3. **T√πy ch·ªânh** theo nhu c·∫ßu c·ª• th·ªÉ c·ªßa d·ª± √°n
4. **Kh√¥ng ph·ª• thu·ªôc** v√†o th∆∞ vi·ªán NLP c√≥ s·∫µn

ƒê√¢y l√† m·ªôt h·ªá th·ªëng NLP ho√†n ch·ªânh cho t√¨m ki·∫øm phim b·∫±ng gi·ªçng n√≥i! üöÄ
